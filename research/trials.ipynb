{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains import conversational_retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "repo = Repo.clone_from(\"https://github.com/LokeshDangare/Signature-Recognition-System\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the python files from that repo\n",
    "\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,\n",
    "    glob= \"**/*\",\n",
    "    suffixes=[\".py\"],\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from fastapi import FastAPI, File\\nfrom uvicorn import run as app_run\\nfrom fastapi.middleware.cors import CORSMiddleware\\nfrom fastapi.responses import Response, JSONResponse\\nfrom src.constants import APP_HOST, APP_PORT\\nfrom src.pipeline.training import TrainingPipeline\\nfrom src.pipeline.prediction import PredictionPipeline\\n\\napp = FastAPI()\\n\\norigins = [\\'#\\']\\napp.add_middleware(CORSMiddleware, allow_origins=origins, allow_credentials=True, allow_methods=[\\'#\\'],\\n                   allow_headers=[\\'#\\'])\\n\\n\\n@app.get(\"/train\")\\nasync def training():\\n    try:\\n        train_pipeline = TrainingPipeline()\\n        train_pipeline.run_pipeline()\\n        return Response(\"Training Successful !!!\")\\n    except Exception as e:\\n        return Response(f\"Error Occurred!!! {e}\")\\n\\n\\n@app.post(\"/predict\")\\nasync def prediction(image_file: bytes = File(description=\"A file read as bytes\")):\\n    try:\\n        prediction_pipeline = PredictionPipeline()\\n        final_output = prediction_pipeline.run_pipeline(image_file)\\n        return final_output\\n    except Exception as e:\\n        return JSONResponse(content=f\"Error Occurred!!! {e}\", status_code=500)\\n\\n\\nif __name__ == \"__main__\":\\n    app_run(app, host=APP_HOST, port=APP_PORT)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\main.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.pipeline.training import TrainingPipeline\\n\\nif __name__ == \"__main__\":\\n    tracking_pipeline = TrainingPipeline()\\n    tracking_pipeline.run_pipeline()'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import setup, find_packages\\n\\nsetup(\\n    name=\"src\",\\n    version=\"0.0.1\",\\n    author=\"Lokesh\",\\n    author_email=\"lokeshdangare05@gmail.com\",\\n    packages=find_packages(),\\n    install_requires=[]\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nfrom zipfile import ZipFile\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\nfrom src.configurations.gcloud_syncer import GCloudSync\\nfrom src.entity.config_entity import DataIngestionConfig\\nfrom src.entity.artifacts_entity import DataIngestionArtifacts\\n\\nclass DataIngestion:\\n\\n    def __init__(self, data_ingestion_config: DataIngestionConfig):\\n        \"\"\"\\n        param data_ingestion_config: Configuration for data ingestion\\n        \"\"\"\\n        self.data_ingestion_config = data_ingestion_config\\n        self.gcloud = GCloudSync()\\n\\n    def get_data_from_gcloud(self) -> None:\\n        \"\"\"\\n        Method Name: get_data_from_gcloud\\n        Description: This function fetch data from gcloud\\n\\n        Output: Returns data into DataIngestionArtifacts\\n        On Failure: Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the get_data_from_gcloud method of Data ingestion class\")\\n            os.makedirs(self.data_ingestion_config.DATA_INGESTION_ARTIFACTS_DIR, exist_ok=True)\\n            self.gcloud.sync_file_from_gcloud(self.data_ingestion_config.BUCKET_NAME,\\n                                              self.data_ingestion_config.ZIP_FILE_NAME,\\n                                              self.data_ingestion_config.DATA_INGESTION_ARTIFACTS_DIR,)\\n            logging.info(\"Exited the get_data_from_gcloud method of Data ingestion class\")\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def unzip_and_clean(self) -> None:\\n        \"\"\"\\n        Method Name: unzip and clean\\n        Description: This function unzip the dataset\\n\\n        Output: Returns unzipped Data\\n        On Failure: Write an exception log and then raise an exception\\n        \"\"\"\\n        logging.info(\"Entered the unzip_and_clean method of Data ingestion class\")\\n        try:\\n            with ZipFile(self.data_ingestion_config.ZIP_FILE_PATH, \\'r\\') as zip_ref:\\n                zip_ref.extractall(self.data_ingestion_config.DATA_INGESTION_ARTIFACTS_DIR)\\n            logging.info(\"Exited the unzip_and_clean method of Data ingestion class\")\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def initiate_data_ingestion(self) -> DataIngestionArtifacts:\\n        \"\"\"\\n        Method Name: initiate_data_ingestion\\n        Description: This function initiates a data ingestion steps\\n\\n        Output: Returns data ingestion artifacts\\n        On Failure: Write an exception log and then raise an exception\\n        \"\"\"\\n        logging.info(\"Entered the initiate_data_ingestion method of Data ingestion class\")\\n        try:\\n            self.get_data_from_gcloud()\\n            logging.info(\"Fetched the zipped dataset from GCloud Storage Bucket\")\\n\\n            self.unzip_and_clean()\\n            logging.info(\"Unzipped the file fetched from GCloud Storage Bucket\")\\n\\n            logging.info(\"Deleting Signature_data.zip file\")\\n            os.remove(os.path.join(self.data_ingestion_config.DATA_INGESTION_ARTIFACTS_DIR,\\n                                   self.data_ingestion_config.ZIP_FILE_NAME))\\n\\n            data_ingestion_artifacts = DataIngestionArtifacts(\\n                dataset_path = self.data_ingestion_config.DATA_INGESTION_ARTIFACTS_DIR)\\n\\n            logging.info(f\"Data ingestion artifacts: {data_ingestion_artifacts}\")\\n\\n            logging.info(\"Exited the initiate_data_ingestion method of Data ingestion class\")\\n            return data_ingestion_artifacts\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport torch\\nfrom src.logger import logging\\nfrom torchvision import datasets\\nfrom torchvision import transforms as T\\nfrom src.exception import CustomException\\nfrom src.utils.main_utils import save_object\\nfrom src.entity.config_entity import DataTransformationConfig\\nfrom src.entity.artifacts_entity import DataIngestionArtifacts, DataTransformationArtifacts\\n\\nclass DataTransformation:\\n    def __init__(self, data_transformation_config: DataTransformationConfig,\\n                 data_ingestion_artifacts: DataIngestionArtifacts):\\n        \"\"\"\\n\\n        :param data_transformation_config: Configuration for data transformation\\n        :param data_ingestion_artifacts: Artifacts for data ingestion\\n        \"\"\"\\n        self.data_transformation_config = data_transformation_config\\n        self.data_ingestion_artifacts = data_ingestion_artifacts\\n        self.std =self.data_transformation_config.STD\\n        self.mean = self.data_transformation_config.MEAN\\n        self.img_size = self.data_transformation_config.IMG_SIZE\\n        self.degree_n = self.data_transformation_config.DEGREE_N\\n        self.degree_p = self.data_transformation_config.DEGREE_P\\n        self.train_ratio = self.data_transformation_config.TRAIN_RATIO\\n        self.valid_ratio = self.data_transformation_config.VALID_RATIO\\n\\n    def get_transform_data(self):\\n        \"\"\"\\n        :return: transform data\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the get_transform_data method of Data transformation class\")\\n            data_transform = T.Compose([\\n                T.Resize(size=(self.img_size, self.img_size)), #Resizing image to be 224 by 224\\n                T.RandomRotation(degrees=(self.degree_n, self.degree_p)), #Randomly rotate images by +/- 20 degrees, Image Augmentation for each epoch\\n                T.ToTensor(), #Converting dimension from (height,weight,channel) to (channel,height,weight) convention of PyTorch\\n                T.Normalize(self.mean, self.std) #Normalize by 3 means 3 STD\\'s of the imagenet, 3 channels\\n            ])\\n            logging.info(\"Exited the get_transform_data method of Data transformation class\")\\n            return data_transform\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def split_data(self, dataset, total_count):\\n        \"\"\"\\n        Method Name: Split Data\\n        Description: This function split data into train, valid and test\\n\\n        Output: Returns train and test dataset\\n        on Failure: Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the split_data method of Data transformation class\")\\n            train_count = int(self.train_ratio * total_count)\\n            valid_count = int(self.valid_ratio * total_count)\\n            test_count = total_count - train_count - valid_count\\n            train_data, valid_data, test_data = torch.utils.data.random_split(dataset, (train_count, valid_count, test_count))\\n            logging.info(\"Exited the split_data method of Data transformation class\")\\n            return train_data, valid_data, test_data\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def initiate_data_transformation(self) -> DataTransformationArtifacts:\\n        \"\"\"\\n        Method Name: initiate data_transformation\\n        Description: This function initiate a data transformation steps\\n\\n        Output: Returns data transformation artifact\\n        on Failure: Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the initiate_data_transformation method of Data transformation class\")\\n\\n            dataset = datasets.ImageFolder(self.data_ingestion_artifacts.dataset_path,\\n                                           transform=self.get_transform_data())\\n            total_count = len(dataset)\\n            logging.info(f\"Total number of records: {total_count}\")\\n\\n            classes = len(os.listdir(self.data_ingestion_artifacts.dataset_path))\\n            logging.info(f\"Total number of classes: {classes}\")\\n\\n            train_dataset, valid_dataset, test_dataset = self.split_data(dataset, total_count)\\n            logging.info(\"Split dataset into train, valid and test\")\\n\\n            save_object(self.data_transformation_config.TRAIN_TRANSFORM_OBJECT_FILE_PATH, train_dataset)\\n            save_object(self.data_transformation_config.VALID_TRANSFORM_OBJECT_FILE_PATH, valid_dataset)\\n            save_object(self.data_transformation_config.TEST_TRANSFORM_OBJECT_FILE_PATH, test_dataset)\\n            logging.info(\"Saved the train, valid and test transformed object\")\\n\\n            data_transformation_artifact = DataTransformationArtifacts(\\n                train_transformed_object=self.data_transformation_config.TRAIN_TRANSFORM_OBJECT_FILE_PATH,\\n                valid_transformed_object=self.data_transformation_config.VALID_TRANSFORM_OBJECT_FILE_PATH,\\n                test_transformed_object=self.data_transformation_config.TEST_TRANSFORM_OBJECT_FILE_PATH,\\n                classes=classes\\n            )\\n            logging.info(f\"Data transformation artifact: {data_transformation_artifact}\")\\n            logging.info(\"Exied the initiate_data_transformation method of Data transformation class\")\\n            return data_transformation_artifact\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport torch\\nfrom tqdm import tqdm\\nfrom src.logger import logging\\nfrom src.constants import DEVICE\\nfrom torch.utils.data import DataLoader\\nfrom src.exception import CustomException\\nfrom src.utils.main_utils import load_object\\nfrom src.configurations.gcloud_syncer import GCloudSync\\nfrom src.entity.config_entity import ModelEvaluationConfig\\nfrom src.entity.artifacts_entity import ModelTrainerArtifacts, DataTransformationArtifacts, ModelEvaluationArtifacts\\n\\n\\nclass ModelEvaluation:\\n    def __init__(self, model_evaluation_config: ModelEvaluationConfig,\\n                 model_trainer_artifacts: ModelTrainerArtifacts,\\n                 data_transformation_artifacts: DataTransformationArtifacts):\\n        \"\"\"\\n        :param model_evaluation_config: Configuration for model evaluation\\n        :param model_trainer_artifacts: Output reference of model trainer artifact stage\\n        :param data_transformation_artifacts: Output reference of data transformation artifact stage\\n        \"\"\"\\n\\n        self.model_evaluation_config = model_evaluation_config\\n        self.model_trainer_artifacts = model_trainer_artifacts\\n        self.data_transformation_artifacts = data_transformation_artifacts\\n        self.gcloud = GCloudSync()\\n\\n    def get_best_model_from_gcloud(self) -> str:\\n        \"\"\"\\n        :return: Fetch best model from gcloud storage and store inside best model directory path\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the get_best_model_from_gcloud method of Model Evaluation class\")\\n\\n            os.makedirs(self.model_evaluation_config.BEST_MODEL_DIR, exist_ok=True)\\n            self.gcloud.sync_file_from_gcloud(self.model_evaluation_config.BUCKET_NAME,\\n                                              self.model_evaluation_config.MODEL_NAME,\\n                                              self.model_evaluation_config.BEST_MODEL_DIR)\\n            best_model_path = os.path.join(self.model_evaluation_config.BEST_MODEL_DIR,\\n                                           self.model_evaluation_config.MODEL_NAME)\\n            logging.info(\"Exited the get_best_model_from_gcloud method of Model Evaluation class\")\\n            return best_model_path\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def evaluate(self, model, criterion, test_dataloader):\\n        \"\"\"\\n        Model Name: Evaluate\\n        Description: This method takes model, loss function and data loader\\n\\n        Output: Return total loss\\n        \"\"\"\\n        try:\\n            total_test_loss = 0\\n            model.eval()\\n            with tqdm(test_dataloader, unit=\\'batch\\', leave=False) as pbar:\\n                pbar.set_description(f\\'testing\\')\\n                for images, idxs in pbar:\\n                    images = images.to(DEVICE, non_blocking=True)\\n                    idxs = idxs.to(DEVICE, non_blocking=True)\\n                    output = model(images)\\n\\n                    loss = criterion(output, idxs)\\n                    total_test_loss += loss.item()\\n\\n            test_loss = total_test_loss / len(self.data_transformation_artifacts.test_transformed_object)\\n            print(f\\'Test Loss: {test_loss:.4f}\\')\\n            return test_loss\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def initiate_model_evaluation(self) -> ModelEvaluationArtifacts:\\n        \"\"\"\\n        Model Name: Initiate_model_evaluation\\n        Description: This function is used to initiate all steps of the model evaluation\\n\\n        Output: Return model evaluation artifacts\\n        On failure: Write an exception log and then raise an exception\\n        \"\"\"\\n        logging.info(\"Entered the Initiate Model Evaluation\")\\n        try:\\n            logging.info(\"Loading the validation data for model evaluation\")\\n            test_dataset = load_object(self.data_transformation_artifacts.test_transformed_object)\\n            test_loader = DataLoader(test_dataset, shuffle=False,\\n                                     batch_size=self.model_evaluation_config.BATCH_SIZE,\\n                                     num_workers=self.model_evaluation_config.NUM_WORKERS)\\n            criterion = torch.nn.CrossEntropyLoss()\\n\\n            logging.info(\"Loading currently trained model\")\\n            model = torch.load(self.model_trainer_artifacts.trained_model_path, map_location=DEVICE)\\n            model.eval()\\n\\n            trained_model_loss = self.evaluate(model, criterion, test_loader)\\n\\n            logging.info(\"Fetch best model from gcloud storage\")\\n            best_model_path = self.get_best_model_from_gcloud()\\n\\n            logging.info(\"Checked if best model present in gcloud or not ?\")\\n            if os.path.isfile(best_model_path) is False:\\n                is_model_accepted = True\\n                logging.info(\"gcloud storage model is false and currently trained model accepted is true\")\\n            else:\\n                logging.info(\"Loading best model fetched from gcloud storage\")\\n                model = torch.load(best_model_path, map_location=DEVICE)\\n                model.eval()\\n                best_model_loss = self.evaluate(model, criterion, test_loader)\\n\\n                logging.info(\"Comparing loss between best_model_loss and trained_model_loss ? \")\\n                if best_model_loss > trained_model_loss:\\n                    is_model_accepted = True\\n                    logging.info(\"Trained model not accepted\")\\n                else:\\n                    is_model_accepted = False\\n                    logging.info(\"Trained model not accepted\")\\n\\n            model_evaluation_artifacts = ModelEvaluationArtifacts(is_model_accepted=is_model_accepted)\\n            return model_evaluation_artifacts\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_pusher.py', 'language': <Language.PYTHON: 'python'>}, page_content='import sys\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\nfrom src.configurations.gcloud_syncer import GCloudSync\\nfrom src.entity.config_entity import ModelPusherConfig\\nfrom src.entity.artifacts_entity import ModelTrainerArtifacts, ModelPusherArtifacts\\n\\n\\nclass ModelPusher:\\n    def __init__(self, model_pusher_config: ModelPusherConfig,\\n                 model_trainer_artifacts: ModelTrainerArtifacts):\\n        \"\"\"\\n        :param model_pusher_config: Configuration for model pusher\\n        :param model_trainer_artifacts: Output reference of model trainer artifact stage\\n        \"\"\"\\n        self.model_pusher_config = model_pusher_config\\n        self.model_trainer_artifacts = model_trainer_artifacts\\n        self.gcloud = GCloudSync()\\n\\n    def initiate_model_pusher(self) -> ModelPusherArtifacts:\\n        \"\"\"\\n        Method Name: Initiate_model_pusher\\n        Description: This method initiate model pusher\\n        Output: Model trainer artifacts\\n        \"\"\"\\n        logging.info(\"Entered the initiate_model_pusher method of ModelPusher class\")\\n        try:\\n            logging.info(\"Uploading the model to gcloud storage\")\\n            self.gcloud.sync_file_to_gcloud(self.model_pusher_config.BUCKET_NAME,self.model_trainer_artifacts.trained_model_path)\\n            logging.info(\"Uploaded the best model to gcloud storage\")\\n\\n            logging.info(\"Saving the model pusher artifacts\")\\n            model_pusher_artifacts = ModelPusherArtifacts(bucket_name=self.model_pusher_config.BUCKET_NAME)\\n            logging.info(\"Exited the initiate_model_pusher method of ModelPusher class\")\\n            return model_pusher_artifacts\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport torch\\nfrom tqdm import tqdm\\nimport torch.nn as nn\\nfrom torchvision import models\\nfrom src.logger import logging\\nfrom src.constants import DEVICE\\nfrom torch.utils.data import DataLoader\\nfrom src.exception import CustomException\\nfrom src.utils.main_utils import load_object\\nfrom src.entity.config_entity import ModelTrainerConfig\\nfrom src.entity.artifacts_entity import DataTransformationArtifacts, ModelTrainerArtifacts\\n\\n\\nclass ModelTrainer:\\n    def __init__(self, model_trainer_config: ModelTrainerConfig,\\n                 data_transformation_artifacts: DataTransformationArtifacts):\\n        \"\"\"\\n        :param model_trainer_config: Configuration for model trainer\\n        :param data_transformation_artifacts: Artifacts for data transformation\\n        \"\"\"\\n        self.model_trainer_config = model_trainer_config\\n        self.data_transformation_artifacts = data_transformation_artifacts\\n        self.learning_rate = self.model_trainer_config.LR\\n        self.epochs = self.model_trainer_config.EPOCHS\\n        self.batch_size = self.model_trainer_config.BATCH_SIZE\\n        self.num_workers = self.model_trainer_config.NUM_WORKERS\\n\\n    def train(self, model, criterion, optimizer, train_dataloader, valid_dataloader):\\n        \"\"\"\\n        Method Name: train\\n        Description: This method takes pretrained model, loss, optimizer, train and valid data laoder\\n        to start training\\n        \"\"\"\\n        try:\\n            total_train_loss = 0\\n            total_test_loss = 0\\n\\n            model.train()\\n            with tqdm(train_dataloader, unit=\\'batch\\', leave=False) as pbar:\\n                pbar.set_description(f\\'training\\')\\n                for images, idxs in pbar:\\n                    images = images.to(DEVICE, non_blocking=True)\\n                    idxs = idxs.to(DEVICE, non_blocking=True)\\n                    output = model(images)\\n\\n                    loss = criterion(output, idxs)\\n                    total_train_loss += loss.item()\\n\\n                    loss.backward()\\n                    optimizer.step()\\n                    optimizer.zero_grad(set_to_none=True)\\n\\n            model.eval()\\n            with tqdm(valid_dataloader, unit=\\'batch\\', leave=False) as pbar:\\n                pbar.set_description(f\\'testing\\')\\n                for images, idxs in pbar:\\n                    images = images.to(DEVICE, non_blocking=True)\\n                    idxs = idxs.to(DEVICE, non_blocking=True)\\n                    output = model(images)\\n\\n                    loss = criterion(output, idxs)\\n                    total_test_loss += loss.item()\\n\\n            train_loss = total_train_loss / len(self.data_transformation_artifacts.train_transformed_object)\\n            valid_loss = total_test_loss / len(self.data_transformation_artifacts.valid_transformed_object)\\n            print(f\\'Train Loss: {train_loss:.4f} Test Loss: {valid_loss:.4f}\\')\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n\\n    def initiate_model_trainer(self) -> ModelTrainerArtifacts:\\n        \"\"\"\\n        Method Name: initiate_model_trainer\\n        Description: This method initiate model trainer steps\\n\\n        Output: Return model trainer artifacts\\n        On Failur: Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the initiate_model_trainer method of Model trainer class\")\\n\\n            train_dataset = load_object(self.data_transformation_artifacts.train_transformed_object)\\n            valid_dataset = load_object(self.data_transformation_artifacts.valid_transformed_object)\\n\\n            logging.info(\"Loaded dataset from data transformation artifacts\")\\n            train_loader = DataLoader(train_dataset, shuffle=True, batch_size=self.batch_size, num_workers=self.num_workers)\\n            valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=self.batch_size, num_workers=self.num_workers)\\n            logging.info(\"Loaded train and valid data loader\")\\n\\n            model = models.resnet34(weights=\\'ResNet34_Weights.DEFAULT\\')\\n            logging.info(\"Loaded pretrained resnet34 model\")\\n\\n            model.fc = nn.Sequential(\\n                nn.Dropout(0.1),\\n                nn.Linear(model.fc.in_features, self.data_transformation_artifacts.classes)\\n            )\\n            logging.info(\"Updated the last layer of pretrained model\")\\n\\n            model = model.to(DEVICE)\\n\\n            criterion = torch.nn.CrossEntropyLoss()\\n            logging.info(\"Cross entropy loss function is used.\")\\n\\n            optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.9)\\n            logging.info(\"SGD optimizer is used.\")\\n\\n            logging.info(\"Model Training Started\")\\n            for i in range(self.epochs):\\n                logging.info(f\"Model training at epoch: {i+1}\")\\n                print(f\"Epoch: {i+1}/{self.epochs}\")\\n                self.train(model, criterion, optimizer, train_loader, valid_loader)\\n            logging.info(\"Model Training Done!!!\")\\n\\n            os.makedirs(self.model_trainer_config.MODEL_TRAINER_ARTIFACTS_DIR, exist_ok=True)\\n            torch.save(model, self.model_trainer_config.TRAINED_MODEL_PATH)\\n            logging.info(f\"Saved trained model at {self.model_trainer_config.TRAINED_MODEL_PATH}\")\\n\\n            model_trainer_artifacts = ModelTrainerArtifacts(\\n                trained_model_path=self.model_trainer_config.TRAINED_MODEL_PATH)\\n            logging.info(f\"Model trainer artifacts: {model_trainer_artifacts}\")\\n\\n            logging.info(\"Exited the initiate_model_trainer method of Model trainer class\")\\n            return model_trainer_artifacts\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\configurations\\\\gcloud_syncer.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nfrom src.exception import CustomException\\n\\nclass GCloudSync:\\n    def sync_file_from_gcloud(self, gcp_bucket_url, filename, destination):\\n        \"\"\"\\n        param gcp_bucket_url: GCP bucket url\\n        param filepath: filepath\\n        param destination: where to store\\n        \"\"\"\\n        try:\\n            command = f\"gsutil cp gs://{gcp_bucket_url}/{filename} {destination}/\"\\n            #command = f\"gcloud storage cp gs://{gcp_bucket_url}/{filename} {destination}/{filename}\"\\n            os.system(command)\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def sync_file_to_gcloud(self, gcp_bucket_url, filepath):\\n        \"\"\"\\n        param gcp_bucket_url: GCP bucket url\\n        param filepath: filepath\\n        \"\"\"\\n        try:\\n            command = f\"gsutil cp {filepath} gs://{gcp_bucket_url}/\"\\n            #command = f\"gcloud storage cp {filepath}/{filename} gs://{gcp_bucket_url}/\"\\n            os.system(command)\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\configurations\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\constants\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport torch\\nfrom datetime import datetime\\n\\n#Common Constants\\n\\nCONFIG_PATH: str = os.path.join(os.getcwd(), \"config\", \"config.yaml\")\\nTIMESTAMP: str = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\\nARTIFACTS_DIR = os.path.join(\"artifacts\", TIMESTAMP)\\nuse_cuda = torch.cuda.is_available()\\nDEVICE = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\\n\\n#FastAPI Constants\\nAPP_HOST = \"127.0.0.1\"\\nAPP_PORT =8080\\n\\n#Data Ingestion Constants\\nDATA_INGESTION_ARTIFACTS_DIR = \\'DataIngestionArtifacts\\'\\n\\n#Data Transformation Constants\\nDATA_TRANSFORMATION_ARTIFACTS_DIR = \\'DataTransformationArtifacts\\'\\nDATA_TRANSFORMATION_TRAIN_FILE_NAME = \"train_transformed.pkl\"\\nDATA_TRANSFORMATION_VALID_FILE_NAME = \"valid_transformed.pkl\"\\nDATA_TRANSFORMATION_TEST_FILE_NAME = \"test_transformed.pkl\"\\n\\n#Model Trainer Constants\\nMODEL_TRAINER_ARTIFACTS_DIR = \"ModelTrainerArtifacts\"\\nTRAINED_MODEL_PATH = \"model.pt\"\\n\\n#Model Evaluation Constants\\nMODEL_EVALUATION_ARTIFACTS_DIR = \"ModelEvaluationArtifacts\"\\nBEST_MODEL_DIR = \"best_model\"\\nMODEL_NAME = \"model.pt\"\\n\\n#Prediction Pipeline\\nLABEL_NAME = [\\'Forged\\', \\'Original\\']'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\artifacts_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='from dataclasses import dataclass\\n\\n#Data Ingestion Artifacts\\n@dataclass\\nclass DataIngestionArtifacts:\\n    dataset_path: str\\n\\n    def to_dict(self):\\n        return self.__dict__\\n\\n#Data Transformation Artifacts\\n@dataclass\\nclass DataTransformationArtifacts:\\n    train_transformed_object: str\\n    valid_transformed_object: str\\n    test_transformed_object: str\\n    classes: int\\n\\n    def to_dict(self):\\n        return self.__dict__\\n\\n#Model Trainer Artifacts\\n@dataclass\\nclass ModelTrainerArtifacts:\\n    trained_model_path: str\\n\\n    def to_dict(self):\\n        return self.__dict__\\n\\n#Model Evaluation Artifacts:\\n@dataclass\\nclass ModelEvaluationArtifacts:\\n    is_model_accepted: bool\\n\\n    def to_dict(self):\\n        return self.__dict__\\n\\n#Model Pusher Artifacts\\n@dataclass\\nclass ModelPusherArtifacts:\\n    bucket_name: str\\n\\n    def to_dict(self):\\n        return self.__dict__'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom src.constants import *\\nfrom dataclasses import dataclass\\nfrom src.utils.main_utils import read_yaml_file\\n\\n@dataclass\\nclass DataIngestionConfig:\\n    def __init__(self):\\n        self.config = read_yaml_file(CONFIG_PATH)\\n        self.BUCKET_NAME: str = self.config[\\'data_ingestion_config\\'][\"bucket_name\"]\\n        self.ZIP_FILE_NAME: str = self.config[\\'data_ingestion_config\\'][\"zip_file_name\"]\\n        self.DATA_INGESTION_ARTIFACTS_DIR: str = os.path.join(os.getcwd(), ARTIFACTS_DIR, DATA_INGESTION_ARTIFACTS_DIR)\\n        self.ZIP_FILE_PATH: str = os.path.join(self.DATA_INGESTION_ARTIFACTS_DIR, self.ZIP_FILE_NAME)\\n\\nclass DataTransformationConfig:\\n    def __init__(self):\\n        self.config = read_yaml_file(CONFIG_PATH)\\n        self.STD: list = self.config[\\'data_transformation_config\\'][\"std\"]\\n        self.MEAN: list = self.config[\\'data_transformation_config\\'][\"mean\"]\\n        self.IMG_SIZE: int = self.config[\\'data_transformation_config\\'][\"img_size\"]\\n        self.DEGREE_N: int = self.config[\\'data_transformation_config\\'][\"degree_n\"]\\n        self.DEGREE_P: int = self.config[\\'data_transformation_config\\'][\"degree_p\"]\\n        self.TRAIN_RATIO: float = self.config[\\'data_transformation_config\\'][\"train_ratio\"]\\n        self.VALID_RATIO: float = self.config[\\'data_transformation_config\\'][\"valid_ratio\"]\\n        self.DATA_TRANSFORMATION_ARTIFACTS_DIR: str = os.path.join(os.getcwd(), ARTIFACTS_DIR, DATA_TRANSFORMATION_ARTIFACTS_DIR)\\n        self.TRAIN_TRANSFORM_OBJECT_FILE_PATH: str = os.path.join(self.DATA_TRANSFORMATION_ARTIFACTS_DIR, DATA_TRANSFORMATION_TRAIN_FILE_NAME)\\n        self.VALID_TRANSFORM_OBJECT_FILE_PATH: str = os.path.join(self.DATA_TRANSFORMATION_ARTIFACTS_DIR, DATA_TRANSFORMATION_VALID_FILE_NAME)\\n        self.TEST_TRANSFORM_OBJECT_FILE_PATH: str = os.path.join(self.DATA_TRANSFORMATION_ARTIFACTS_DIR, DATA_TRANSFORMATION_TEST_FILE_NAME)\\n\\n@dataclass\\nclass ModelTrainerConfig:\\n    def __init__(self):\\n        self.config = read_yaml_file(CONFIG_PATH)\\n        self.LR: float = self.config[\\'model_trainer_config\\'][\\'lr\\']\\n        self.EPOCHS: int = self.config[\\'model_trainer_config\\'][\\'epochs\\']\\n        self.NUM_WORKERS: int = self.config[\\'model_trainer_config\\'][\\'num_workers\\']\\n        self.BATCH_SIZE: int = self.config[\\'model_trainer_config\\'][\\'batch_size\\']\\n        self.MODEL_TRAINER_ARTIFACTS_DIR: str = os.path.join(os.getcwd(), ARTIFACTS_DIR, MODEL_TRAINER_ARTIFACTS_DIR)\\n        self.TRAINED_MODEL_PATH: str = os.path.join(self.MODEL_TRAINER_ARTIFACTS_DIR, TRAINED_MODEL_PATH)\\n\\n\\n@dataclass\\nclass ModelEvaluationConfig:\\n    def __init__(self):\\n        self.config = read_yaml_file(CONFIG_PATH)\\n        self.MODEL_NAME: str = MODEL_NAME\\n        self.BUCKET_NAME: str = self.config[\\'model_evaluation_config\\'][\"bucket_name\"]\\n        self.BATCH_SIZE: int = self.config[\\'model_evaluation_config\\'][\"batch_size\"]\\n        self.NUM_WORKERS: int = self.config[\\'model_evaluation_config\\'][\"num_workers\"]\\n        self.MODEL_EVALUATION_ARTIFACTS_DIR: str = os.path.join(os.getcwd(), ARTIFACTS_DIR, MODEL_EVALUATION_ARTIFACTS_DIR)\\n        self.BEST_MODEL_DIR: str = os.path.join(self.MODEL_EVALUATION_ARTIFACTS_DIR, BEST_MODEL_DIR)\\n\\n\\n@dataclass\\nclass ModelPusherConfig:\\n    def __init__(self):\\n        self.config = read_yaml_file(CONFIG_PATH)\\n        self.MODEL_NAME: str = MODEL_NAME\\n        self.BUCKET_NAME: str = self.config[\\'model_pusher_config\\'][\"bucket_name\"]\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\exception\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='import sys\\n\\n\\ndef error_message_detail(error, error_details: sys):\\n    _, _, exc_tb = error_details.exc_info()\\n    file_name = exc_tb.tb_frame.f_code.co_filename\\n    error_message = \"Error occurred python script name [{0}] line number [{1}] error message[{2}]\".format(\\n        file_name, exc_tb.tb_lineno, str(error)\\n    )\\n\\n    return error_message\\n\\n\\nclass CustomException(Exception):\\n    def __init__(self, error_message, error_detail):\\n        super().__init__(error_message)\\n        self.error_message = error_message_detail(\\n            error_message, error_details=error_detail\\n        )\\n\\n    def __str__(self):\\n        return self.error_message\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\logger\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='import logging\\nimport os\\nfrom datetime import datetime\\n\\nLOG_FILE_NAME = f\"{datetime.now().strftime(\\'%m_%d_%Y_%H_%M_%S\\')}.log\"\\n\\nos.makedirs(os.path.join(os.getcwd(), \"logs\"), exist_ok=True)\\n\\nlogs_dir_path = os.path.join(os.getcwd(), \"logs\")\\n\\nLOG_FILE_PATH = os.path.join(logs_dir_path, LOG_FILE_NAME)\\n\\nlogging.basicConfig(\\n    filename=LOG_FILE_PATH,\\n    format=\"[ %(asctime)s ] %(name)s - %(levelname)s - %(message)s\",\\n    level=logging.INFO,\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport torch\\nfrom PIL import Image\\nfrom src.constants import *\\nfrom src.logger import logging\\nfrom torchvision import transforms\\nfrom src.exception import CustomException\\nfrom src.utils.main_utils import read_yaml_file\\nfrom src.configurations.gcloud_syncer import GCloudSync\\n\\n\\nclass PredictionPipeline:\\n\\n    def __init__(self):\\n        self.gcloud = GCloudSync()\\n        self.config = read_yaml_file(CONFIG_PATH)\\n        self.img_size = self.config[\\'data_transformation_config\\'][\\'img_size\\']\\n\\n    def image_loader(self, image_bytes):\\n        \"\"\"\\n        Method Name: Image loader\\n        Description: This method load byte image and save it to local\\n        Output: Returns path of the saved image        :\\n        \"\"\"\\n        logging.info(\"Entered the image_loader method of PredictionPipeline class\")\\n        try:\\n            logging.info(\"load byte image and save it to local\")\\n            input_image = self.config[\\'prediction_pipeline_config\\'][\\'input_image\\']\\n            with open(input_image, \\'wb\\') as image:\\n                image.write(image_bytes)\\n                image.close()\\n            path = os.path.join(os.getcwd(), input_image)\\n            image = Image.open(path)\\n            logging.info(f\"Returns the saved image: {image}\")\\n            logging.info(\"Exited the image_loader method of PredictionPipeline class\")\\n            return image\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def get_model_from_gcloud(self) -> str:\\n        \"\"\"\\n        Method Name: Get_model_from_gcloud\\n        Description: This method fetched the best model from the gcloud\\n        Output: Return best model path\\n        \"\"\"\\n        logging.info(\"Entered the get_model_from_gcloud method of PredictionPipeline class\")\\n        try:\\n            logging.info(\"Loading the best model from gcloud bucket\")\\n            os.makedirs(\"artifacts/PredictModel\", exist_ok=True)\\n            predict_model_path = os.path.join(os.getcwd(), \"artifacts\", \"PredictModel\")\\n            self.gcloud.sync_file_from_gcloud(self.config[\\'prediction_pipeline_config\\'][\"bucket_name\"],\\n                                              self.config[\\'prediction_pipeline_config\\'][\"model_name\"],\\n                                              predict_model_path)\\n            best_model_path = os.path.join(predict_model_path, self.config[\\'prediction_pipeline_config\\'][\"model_name\"])\\n            logging.info(\"Exited the get_model_from_gcloud method of PredictionPipeline class\")\\n            return best_model_path\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def prediction(self, best_model_path: str, image) -> float:\\n        \"\"\"\\n        Method Name: Prediction\\n        Description: This method takes the best model path and image\\n        Output: Return the image in base64\\n        \"\"\"\\n        logging.info(\"Entered the prediction method of PredictionPipeline class\")\\n        try:\\n            logging.info(\"Loading best model\")\\n            model = torch.load(best_model_path, map_location=DEVICE)\\n            model.eval()\\n\\n            logging.info(\"Load the image and preprocess it\")\\n            preprocess = transforms.Compose([\\n                transforms.Resize(size=(self.img_size, self.img_size)),\\n                transforms.Grayscale(3),\\n                transforms.ToTensor()\\n            ])\\n            image = preprocess(image)\\n            image = image[:3]\\n\\n            logging.info(\"Convert image to a PyTorch tensor and sent it to the device\")\\n            image = image.unsqueeze(0).to(DEVICE)\\n\\n            logging.info(\"Make the prediction\")\\n            with torch.no_grad():\\n                logits = model(image)\\n                probs = torch.softmax(logits, dim=1)\\n                pred_label = torch.argmax(probs, dim=1)\\n\\n            logging.info(f\\'Predicted label: {pred_label.item()}\\')\\n            logging.info(\"Map the predicted label to the corresponding class name\")\\n            predicted_class_name = LABEL_NAME[pred_label.item()]\\n            logging.info(f\\'Predicted class name: {predicted_class_name}\\')\\n            logging.info(\"Exited the prediction method of PredictionPipeline class\")\\n            return predicted_class_name\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def run_pipeline(self, data):\\n        logging.info(\"Entered the run_pipeline method of PredictionPipeline class\")\\n        try:\\n            image = self.image_loader(data)\\n            best_model_path: str = self.get_model_from_gcloud()\\n            detected_image = self.prediction(best_model_path, image)\\n            logging.info(\"Exited the run_pipeline method of PredictionPipeline class\")\\n            return detected_image\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\nfrom src.components.data_ingestion import DataIngestion\\nfrom src.components.data_transformation import DataTransformation\\nfrom src.components.model_trainer import ModelTrainer\\nfrom src.components.model_evaluation import ModelEvaluation\\nfrom src.components.model_pusher import ModelPusher\\nfrom src.entity.config_entity import DataIngestionConfig, DataTransformationConfig, ModelTrainerConfig, ModelEvaluationConfig, ModelPusherConfig\\nfrom src.entity.artifacts_entity import DataIngestionArtifacts, DataTransformationArtifacts, ModelTrainerArtifacts, ModelEvaluationArtifacts, ModelPusherArtifacts\\n\\nclass TrainingPipeline:\\n    def __init__(self):\\n        self.data_ingestion_config = DataIngestionConfig()\\n        self.data_transformation_config = DataTransformationConfig()\\n        self.model_trainer_config = ModelTrainerConfig()\\n        self.model_evaluation_config = ModelEvaluationConfig()\\n        self.model_pusher_config = ModelPusherConfig()\\n\\n    def start_data_ingestion(self) -> DataIngestionArtifacts:\\n        logging.info(\"Entered the start_data_ingestion method of TrainingPipeline class\")\\n        try:\\n            logging.info(\"Getting the dataset from GCloud Storage Bucket\")\\n            data_ingestion = DataIngestion(\\n                data_ingestion_config=self.data_ingestion_config\\n            )\\n            data_ingestion_artifacts = data_ingestion.initiate_data_ingestion()\\n            logging.info(\"Got the dataset from GCloud Storage\")\\n            logging.info(\"Exited the start_data_ingestion method of TrainingPipeline class\")\\n            return data_ingestion_artifacts\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def start_data_transformation(self, data_ingestion_artifacts: DataIngestionArtifacts) -> DataTransformationArtifacts:\\n        logging.info(\"Entered the start_data_transformation method of TrainingPipeline class\")\\n        try:\\n            data_transformation = DataTransformation(\\n                data_ingestion_artifacts=data_ingestion_artifacts,\\n                data_transformation_config=self.data_transformation_config,\\n            )\\n            data_transformation_artifacts = (data_transformation.initiate_data_transformation())\\n            logging.info(\"Exited the start_data_transformation method of TrainingPipeline class\")\\n            return data_transformation_artifacts\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def start_model_trainer(self, data_transformation_artifacts: DataTransformationArtifacts) -> ModelTrainerArtifacts:\\n        logging.info(\"Entered the start_model_trainer method of TrainingPipeline class\")\\n        try:\\n            model_trainer = ModelTrainer(\\n                data_transformation_artifacts=data_transformation_artifacts,\\n                model_trainer_config = self.model_trainer_config,\\n            )\\n            model_trainer_artifacts = (model_trainer.initiate_model_trainer())\\n            logging.info(\"Exited the start_model_trainer method of TrainingPipeline class\")\\n            return model_trainer_artifacts\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def start_model_evaluation(self, model_trainer_artifacts: ModelTrainerArtifacts,\\n                               data_transformation_artifacts: DataTransformationArtifacts) -> ModelEvaluationArtifacts:\\n        logging.info(\"Entered the start_model_evaluation method of TrainingPipeline class\")\\n        try:\\n            model_evaluation = ModelEvaluation(\\n                model_evaluation_config=self.model_evaluation_config,\\n                data_transformation_artifacts=data_transformation_artifacts,\\n                model_trainer_artifacts=model_trainer_artifacts\\n            )\\n            model_evaluation_artifacts = model_evaluation.initiate_model_evaluation()\\n            logging.info(\"Exited the start_model_evaluation method of TrainingPipeline class\")\\n            return model_evaluation_artifacts\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def start_model_pusher(self, model_trainer_artifacts: ModelTrainerArtifacts) -> ModelPusherArtifacts:\\n        logging.info(\"Entered the start_model_pusher method of TrainingPipeline class\")\\n        try:\\n            model_pusher = ModelPusher(model_pusher_config=self.model_pusher_config, model_trainer_artifacts=model_trainer_artifacts)\\n            model_pusher_artifacts = model_pusher.initiate_model_pusher()\\n            logging.info(\"Initiated the model pusher\")\\n            logging.info(\"Exited the start_model_pusher method of TrainingPipeline class\")\\n            return model_pusher_artifacts\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def run_pipeline(self) -> None:\\n        logging.info(\"Entered the run_pipeline method of TrainingPipeline class\")\\n        try:\\n            data_ingestion_artifacts = self.start_data_ingestion()\\n            data_transformation_artifacts = self.start_data_transformation(data_ingestion_artifacts=data_ingestion_artifacts)\\n            model_trainer_artifacts = self.start_model_trainer(data_transformation_artifacts=data_transformation_artifacts)\\n            model_evaluation_artifacts =self.start_model_evaluation(model_trainer_artifacts=model_trainer_artifacts, data_transformation_artifacts=data_transformation_artifacts)\\n            if not model_evaluation_artifacts.is_model_accepted:\\n                raise Exception(\"Trained model is not better than the best model\")\\n            model_pusher_artifacts = self.start_model_pusher(model_trainer_artifacts=model_trainer_artifacts)\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\utils\\\\main_utils.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport dill\\nimport yaml\\nimport base64\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\n\\ndef save_object(file_path: str, obj: object) -> None:\\n    logging.info(\"Entered the save object method of utils\")\\n    try:\\n        os.makedirs(os.path.dirname(file_path),exist_ok=True)\\n        with open(file_path, \"wb\") as file_obj:\\n            dill.dump(obj, file_obj)\\n        logging.info(\"Exited the save_object method of utils\")\\n    except Exception as e:\\n        raise CustomException(e, sys) from e\\n\\ndef load_object(file_path: str) -> object:\\n    logging.info(\"Entered the load_object method of utils\")\\n    try:\\n        with open(file_path, \"rb\") as file_obj:\\n            obj = dill.load(file_obj)\\n        logging.info(\"Exited the load_object method of utils\")\\n        return obj\\n    except Exception as e:\\n        raise CustomException(e, sys) from e\\n\\ndef image_to_base64(image):\\n    try:\\n        logging.info(\"Entered the image_to_base64 method of utils\")\\n        with open(image, \"rb\") as img_file:\\n            my_string = base64.b64encode(img_file.read())\\n        logging.info(\"Exited the image_to_base64 method of utils\")\\n        return my_string\\n    except Exception as e:\\n        raise CustomException(e, sys) from e\\n\\ndef read_yaml_file(file_path: str) -> dict:\\n    try:\\n        with open(file_path, \\'rb\\') as yaml_file:\\n            return yaml.safe_load(yaml_file)\\n    except Exception as e:\\n        raise CustomException(e, sys) from e\\n\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\utils\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from fastapi import FastAPI, File\\nfrom uvicorn import run as app_run\\nfrom fastapi.middleware.cors import CORSMiddleware\\nfrom fastapi.responses import Response, JSONResponse\\nfrom src.constants import APP_HOST, APP_PORT\\nfrom src.pipeline.training import TrainingPipeline\\nfrom src.pipeline.prediction import PredictionPipeline\\n\\napp = FastAPI()\\n\\norigins = [\\'#\\']\\napp.add_middleware(CORSMiddleware, allow_origins=origins, allow_credentials=True, allow_methods=[\\'#\\'],\\n                   allow_headers=[\\'#\\'])\\n\\n\\n@app.get(\"/train\")\\nasync def training():\\n    try:\\n        train_pipeline = TrainingPipeline()\\n        train_pipeline.run_pipeline()\\n        return Response(\"Training Successful !!!\")\\n    except Exception as e:\\n        return Response(f\"Error Occurred!!! {e}\")\\n\\n\\n@app.post(\"/predict\")\\nasync def prediction(image_file: bytes = File(description=\"A file read as bytes\")):\\n    try:\\n        prediction_pipeline = PredictionPipeline()\\n        final_output = prediction_pipeline.run_pipeline(image_file)\\n        return final_output\\n    except Exception as e:\\n        return JSONResponse(content=f\"Error Occurred!!! {e}\", status_code=500)\\n\\n\\nif __name__ == \"__main__\":\\n    app_run(app, host=APP_HOST, port=APP_PORT)\\n')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from fastapi import FastAPI, File\\nfrom uvicorn import run as app_run\\nfrom fastapi.middleware.cors import CORSMiddleware\\nfrom fastapi.responses import Response, JSONResponse\\nfrom src.constants import APP_HOST, APP_PORT\\nfrom src.pipeline.training import TrainingPipeline\\nfrom src.pipeline.prediction import PredictionPipeline\\n\\napp = FastAPI()\\n\\norigins = ['#']\\napp.add_middleware(CORSMiddleware, allow_origins=origins, allow_credentials=True, allow_methods=['#'],\\n                   allow_headers=['#'])\"),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='@app.get(\"/train\")\\nasync def training():\\n    try:\\n        train_pipeline = TrainingPipeline()\\n        train_pipeline.run_pipeline()\\n        return Response(\"Training Successful !!!\")\\n    except Exception as e:\\n        return Response(f\"Error Occurred!!! {e}\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='@app.post(\"/predict\")\\nasync def prediction(image_file: bytes = File(description=\"A file read as bytes\")):\\n    try:\\n        prediction_pipeline = PredictionPipeline()\\n        final_output = prediction_pipeline.run_pipeline(image_file)\\n        return final_output\\n    except Exception as e:\\n        return JSONResponse(content=f\"Error Occurred!!! {e}\", status_code=500)\\n\\n\\nif __name__ == \"__main__\":\\n    app_run(app, host=APP_HOST, port=APP_PORT)'),\n",
       " Document(metadata={'source': 'test_repo\\\\main.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.pipeline.training import TrainingPipeline\\n\\nif __name__ == \"__main__\":\\n    tracking_pipeline = TrainingPipeline()\\n    tracking_pipeline.run_pipeline()'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import setup, find_packages\\n\\nsetup(\\n    name=\"src\",\\n    version=\"0.0.1\",\\n    author=\"Lokesh\",\\n    author_email=\"lokeshdangare05@gmail.com\",\\n    packages=find_packages(),\\n    install_requires=[]\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nfrom zipfile import ZipFile\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\nfrom src.configurations.gcloud_syncer import GCloudSync\\nfrom src.entity.config_entity import DataIngestionConfig\\nfrom src.entity.artifacts_entity import DataIngestionArtifacts'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='class DataIngestion:\\n\\n    def __init__(self, data_ingestion_config: DataIngestionConfig):\\n        \"\"\"\\n        param data_ingestion_config: Configuration for data ingestion\\n        \"\"\"\\n        self.data_ingestion_config = data_ingestion_config\\n        self.gcloud = GCloudSync()\\n\\n    def get_data_from_gcloud(self) -> None:\\n        \"\"\"\\n        Method Name: get_data_from_gcloud\\n        Description: This function fetch data from gcloud'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='Output: Returns data into DataIngestionArtifacts\\n        On Failure: Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the get_data_from_gcloud method of Data ingestion class\")\\n            os.makedirs(self.data_ingestion_config.DATA_INGESTION_ARTIFACTS_DIR, exist_ok=True)\\n            self.gcloud.sync_file_from_gcloud(self.data_ingestion_config.BUCKET_NAME,'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='self.data_ingestion_config.ZIP_FILE_NAME,\\n                                              self.data_ingestion_config.DATA_INGESTION_ARTIFACTS_DIR,)\\n            logging.info(\"Exited the get_data_from_gcloud method of Data ingestion class\")\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='def unzip_and_clean(self) -> None:\\n        \"\"\"\\n        Method Name: unzip and clean\\n        Description: This function unzip the dataset'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='Output: Returns unzipped Data\\n        On Failure: Write an exception log and then raise an exception\\n        \"\"\"\\n        logging.info(\"Entered the unzip_and_clean method of Data ingestion class\")\\n        try:\\n            with ZipFile(self.data_ingestion_config.ZIP_FILE_PATH, \\'r\\') as zip_ref:\\n                zip_ref.extractall(self.data_ingestion_config.DATA_INGESTION_ARTIFACTS_DIR)\\n            logging.info(\"Exited the unzip_and_clean method of Data ingestion class\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='def initiate_data_ingestion(self) -> DataIngestionArtifacts:\\n        \"\"\"\\n        Method Name: initiate_data_ingestion\\n        Description: This function initiates a data ingestion steps'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='Output: Returns data ingestion artifacts\\n        On Failure: Write an exception log and then raise an exception\\n        \"\"\"\\n        logging.info(\"Entered the initiate_data_ingestion method of Data ingestion class\")\\n        try:\\n            self.get_data_from_gcloud()\\n            logging.info(\"Fetched the zipped dataset from GCloud Storage Bucket\")\\n\\n            self.unzip_and_clean()\\n            logging.info(\"Unzipped the file fetched from GCloud Storage Bucket\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Deleting Signature_data.zip file\")\\n            os.remove(os.path.join(self.data_ingestion_config.DATA_INGESTION_ARTIFACTS_DIR,\\n                                   self.data_ingestion_config.ZIP_FILE_NAME))\\n\\n            data_ingestion_artifacts = DataIngestionArtifacts(\\n                dataset_path = self.data_ingestion_config.DATA_INGESTION_ARTIFACTS_DIR)\\n\\n            logging.info(f\"Data ingestion artifacts: {data_ingestion_artifacts}\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Exited the initiate_data_ingestion method of Data ingestion class\")\\n            return data_ingestion_artifacts\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport torch\\nfrom src.logger import logging\\nfrom torchvision import datasets\\nfrom torchvision import transforms as T\\nfrom src.exception import CustomException\\nfrom src.utils.main_utils import save_object\\nfrom src.entity.config_entity import DataTransformationConfig\\nfrom src.entity.artifacts_entity import DataIngestionArtifacts, DataTransformationArtifacts'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='class DataTransformation:\\n    def __init__(self, data_transformation_config: DataTransformationConfig,\\n                 data_ingestion_artifacts: DataIngestionArtifacts):\\n        \"\"\"'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content=':param data_transformation_config: Configuration for data transformation\\n        :param data_ingestion_artifacts: Artifacts for data ingestion\\n        \"\"\"\\n        self.data_transformation_config = data_transformation_config\\n        self.data_ingestion_artifacts = data_ingestion_artifacts\\n        self.std =self.data_transformation_config.STD\\n        self.mean = self.data_transformation_config.MEAN\\n        self.img_size = self.data_transformation_config.IMG_SIZE'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='self.degree_n = self.data_transformation_config.DEGREE_N\\n        self.degree_p = self.data_transformation_config.DEGREE_P\\n        self.train_ratio = self.data_transformation_config.TRAIN_RATIO\\n        self.valid_ratio = self.data_transformation_config.VALID_RATIO'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='def get_transform_data(self):\\n        \"\"\"\\n        :return: transform data\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the get_transform_data method of Data transformation class\")\\n            data_transform = T.Compose([\\n                T.Resize(size=(self.img_size, self.img_size)), #Resizing image to be 224 by 224\\n                T.RandomRotation(degrees=(self.degree_n, self.degree_p)), #Randomly rotate images by +/- 20 degrees, Image Augmentation for each epoch'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='T.ToTensor(), #Converting dimension from (height,weight,channel) to (channel,height,weight) convention of PyTorch\\n                T.Normalize(self.mean, self.std) #Normalize by 3 means 3 STD\\'s of the imagenet, 3 channels\\n            ])\\n            logging.info(\"Exited the get_transform_data method of Data transformation class\")\\n            return data_transform'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def split_data(self, dataset, total_count):\\n        \"\"\"\\n        Method Name: Split Data\\n        Description: This function split data into train, valid and test'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='Output: Returns train and test dataset\\n        on Failure: Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the split_data method of Data transformation class\")\\n            train_count = int(self.train_ratio * total_count)\\n            valid_count = int(self.valid_ratio * total_count)\\n            test_count = total_count - train_count - valid_count'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='train_data, valid_data, test_data = torch.utils.data.random_split(dataset, (train_count, valid_count, test_count))\\n            logging.info(\"Exited the split_data method of Data transformation class\")\\n            return train_data, valid_data, test_data'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='except Exception as e:\\n            raise CustomException(e, sys) from e\\n\\n    def initiate_data_transformation(self) -> DataTransformationArtifacts:\\n        \"\"\"\\n        Method Name: initiate data_transformation\\n        Description: This function initiate a data transformation steps'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='Output: Returns data transformation artifact\\n        on Failure: Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the initiate_data_transformation method of Data transformation class\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='dataset = datasets.ImageFolder(self.data_ingestion_artifacts.dataset_path,\\n                                           transform=self.get_transform_data())\\n            total_count = len(dataset)\\n            logging.info(f\"Total number of records: {total_count}\")\\n\\n            classes = len(os.listdir(self.data_ingestion_artifacts.dataset_path))\\n            logging.info(f\"Total number of classes: {classes}\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='train_dataset, valid_dataset, test_dataset = self.split_data(dataset, total_count)\\n            logging.info(\"Split dataset into train, valid and test\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='save_object(self.data_transformation_config.TRAIN_TRANSFORM_OBJECT_FILE_PATH, train_dataset)\\n            save_object(self.data_transformation_config.VALID_TRANSFORM_OBJECT_FILE_PATH, valid_dataset)\\n            save_object(self.data_transformation_config.TEST_TRANSFORM_OBJECT_FILE_PATH, test_dataset)\\n            logging.info(\"Saved the train, valid and test transformed object\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='data_transformation_artifact = DataTransformationArtifacts(\\n                train_transformed_object=self.data_transformation_config.TRAIN_TRANSFORM_OBJECT_FILE_PATH,\\n                valid_transformed_object=self.data_transformation_config.VALID_TRANSFORM_OBJECT_FILE_PATH,\\n                test_transformed_object=self.data_transformation_config.TEST_TRANSFORM_OBJECT_FILE_PATH,\\n                classes=classes\\n            )'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content=')\\n            logging.info(f\"Data transformation artifact: {data_transformation_artifact}\")\\n            logging.info(\"Exied the initiate_data_transformation method of Data transformation class\")\\n            return data_transformation_artifact'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport torch\\nfrom tqdm import tqdm\\nfrom src.logger import logging\\nfrom src.constants import DEVICE\\nfrom torch.utils.data import DataLoader\\nfrom src.exception import CustomException\\nfrom src.utils.main_utils import load_object\\nfrom src.configurations.gcloud_syncer import GCloudSync\\nfrom src.entity.config_entity import ModelEvaluationConfig\\nfrom src.entity.artifacts_entity import ModelTrainerArtifacts, DataTransformationArtifacts, ModelEvaluationArtifacts'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='class ModelEvaluation:\\n    def __init__(self, model_evaluation_config: ModelEvaluationConfig,\\n                 model_trainer_artifacts: ModelTrainerArtifacts,\\n                 data_transformation_artifacts: DataTransformationArtifacts):\\n        \"\"\"\\n        :param model_evaluation_config: Configuration for model evaluation\\n        :param model_trainer_artifacts: Output reference of model trainer artifact stage'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content=':param data_transformation_artifacts: Output reference of data transformation artifact stage\\n        \"\"\"'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='self.model_evaluation_config = model_evaluation_config\\n        self.model_trainer_artifacts = model_trainer_artifacts\\n        self.data_transformation_artifacts = data_transformation_artifacts\\n        self.gcloud = GCloudSync()'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='def get_best_model_from_gcloud(self) -> str:\\n        \"\"\"\\n        :return: Fetch best model from gcloud storage and store inside best model directory path\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the get_best_model_from_gcloud method of Model Evaluation class\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='os.makedirs(self.model_evaluation_config.BEST_MODEL_DIR, exist_ok=True)\\n            self.gcloud.sync_file_from_gcloud(self.model_evaluation_config.BUCKET_NAME,\\n                                              self.model_evaluation_config.MODEL_NAME,\\n                                              self.model_evaluation_config.BEST_MODEL_DIR)\\n            best_model_path = os.path.join(self.model_evaluation_config.BEST_MODEL_DIR,'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='self.model_evaluation_config.MODEL_NAME)\\n            logging.info(\"Exited the get_best_model_from_gcloud method of Model Evaluation class\")\\n            return best_model_path\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='def evaluate(self, model, criterion, test_dataloader):\\n        \"\"\"\\n        Model Name: Evaluate\\n        Description: This method takes model, loss function and data loader'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='Output: Return total loss\\n        \"\"\"\\n        try:\\n            total_test_loss = 0\\n            model.eval()\\n            with tqdm(test_dataloader, unit=\\'batch\\', leave=False) as pbar:\\n                pbar.set_description(f\\'testing\\')\\n                for images, idxs in pbar:\\n                    images = images.to(DEVICE, non_blocking=True)\\n                    idxs = idxs.to(DEVICE, non_blocking=True)\\n                    output = model(images)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"loss = criterion(output, idxs)\\n                    total_test_loss += loss.item()\\n\\n            test_loss = total_test_loss / len(self.data_transformation_artifacts.test_transformed_object)\\n            print(f'Test Loss: {test_loss:.4f}')\\n            return test_loss\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\"),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='def initiate_model_evaluation(self) -> ModelEvaluationArtifacts:\\n        \"\"\"\\n        Model Name: Initiate_model_evaluation\\n        Description: This function is used to initiate all steps of the model evaluation'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='Output: Return model evaluation artifacts\\n        On failure: Write an exception log and then raise an exception\\n        \"\"\"\\n        logging.info(\"Entered the Initiate Model Evaluation\")\\n        try:\\n            logging.info(\"Loading the validation data for model evaluation\")\\n            test_dataset = load_object(self.data_transformation_artifacts.test_transformed_object)\\n            test_loader = DataLoader(test_dataset, shuffle=False,'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='batch_size=self.model_evaluation_config.BATCH_SIZE,\\n                                     num_workers=self.model_evaluation_config.NUM_WORKERS)\\n            criterion = torch.nn.CrossEntropyLoss()'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Loading currently trained model\")\\n            model = torch.load(self.model_trainer_artifacts.trained_model_path, map_location=DEVICE)\\n            model.eval()\\n\\n            trained_model_loss = self.evaluate(model, criterion, test_loader)\\n\\n            logging.info(\"Fetch best model from gcloud storage\")\\n            best_model_path = self.get_best_model_from_gcloud()'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Checked if best model present in gcloud or not ?\")\\n            if os.path.isfile(best_model_path) is False:\\n                is_model_accepted = True\\n                logging.info(\"gcloud storage model is false and currently trained model accepted is true\")\\n            else:\\n                logging.info(\"Loading best model fetched from gcloud storage\")\\n                model = torch.load(best_model_path, map_location=DEVICE)\\n                model.eval()'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='best_model_loss = self.evaluate(model, criterion, test_loader)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Comparing loss between best_model_loss and trained_model_loss ? \")\\n                if best_model_loss > trained_model_loss:\\n                    is_model_accepted = True\\n                    logging.info(\"Trained model not accepted\")\\n                else:\\n                    is_model_accepted = False\\n                    logging.info(\"Trained model not accepted\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='model_evaluation_artifacts = ModelEvaluationArtifacts(is_model_accepted=is_model_accepted)\\n            return model_evaluation_artifacts\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_pusher.py', 'language': <Language.PYTHON: 'python'>}, page_content='import sys\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\nfrom src.configurations.gcloud_syncer import GCloudSync\\nfrom src.entity.config_entity import ModelPusherConfig\\nfrom src.entity.artifacts_entity import ModelTrainerArtifacts, ModelPusherArtifacts'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_pusher.py', 'language': <Language.PYTHON: 'python'>}, page_content='class ModelPusher:\\n    def __init__(self, model_pusher_config: ModelPusherConfig,\\n                 model_trainer_artifacts: ModelTrainerArtifacts):\\n        \"\"\"\\n        :param model_pusher_config: Configuration for model pusher\\n        :param model_trainer_artifacts: Output reference of model trainer artifact stage\\n        \"\"\"\\n        self.model_pusher_config = model_pusher_config\\n        self.model_trainer_artifacts = model_trainer_artifacts\\n        self.gcloud = GCloudSync()'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_pusher.py', 'language': <Language.PYTHON: 'python'>}, page_content='def initiate_model_pusher(self) -> ModelPusherArtifacts:\\n        \"\"\"\\n        Method Name: Initiate_model_pusher\\n        Description: This method initiate model pusher\\n        Output: Model trainer artifacts\\n        \"\"\"\\n        logging.info(\"Entered the initiate_model_pusher method of ModelPusher class\")\\n        try:\\n            logging.info(\"Uploading the model to gcloud storage\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_pusher.py', 'language': <Language.PYTHON: 'python'>}, page_content='self.gcloud.sync_file_to_gcloud(self.model_pusher_config.BUCKET_NAME,self.model_trainer_artifacts.trained_model_path)\\n            logging.info(\"Uploaded the best model to gcloud storage\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_pusher.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Saving the model pusher artifacts\")\\n            model_pusher_artifacts = ModelPusherArtifacts(bucket_name=self.model_pusher_config.BUCKET_NAME)\\n            logging.info(\"Exited the initiate_model_pusher method of ModelPusher class\")\\n            return model_pusher_artifacts\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport torch\\nfrom tqdm import tqdm\\nimport torch.nn as nn\\nfrom torchvision import models\\nfrom src.logger import logging\\nfrom src.constants import DEVICE\\nfrom torch.utils.data import DataLoader\\nfrom src.exception import CustomException\\nfrom src.utils.main_utils import load_object\\nfrom src.entity.config_entity import ModelTrainerConfig\\nfrom src.entity.artifacts_entity import DataTransformationArtifacts, ModelTrainerArtifacts'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='class ModelTrainer:\\n    def __init__(self, model_trainer_config: ModelTrainerConfig,\\n                 data_transformation_artifacts: DataTransformationArtifacts):\\n        \"\"\"\\n        :param model_trainer_config: Configuration for model trainer\\n        :param data_transformation_artifacts: Artifacts for data transformation\\n        \"\"\"\\n        self.model_trainer_config = model_trainer_config\\n        self.data_transformation_artifacts = data_transformation_artifacts'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='self.learning_rate = self.model_trainer_config.LR\\n        self.epochs = self.model_trainer_config.EPOCHS\\n        self.batch_size = self.model_trainer_config.BATCH_SIZE\\n        self.num_workers = self.model_trainer_config.NUM_WORKERS'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='def train(self, model, criterion, optimizer, train_dataloader, valid_dataloader):\\n        \"\"\"\\n        Method Name: train\\n        Description: This method takes pretrained model, loss, optimizer, train and valid data laoder\\n        to start training\\n        \"\"\"\\n        try:\\n            total_train_loss = 0\\n            total_test_loss = 0'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"model.train()\\n            with tqdm(train_dataloader, unit='batch', leave=False) as pbar:\\n                pbar.set_description(f'training')\\n                for images, idxs in pbar:\\n                    images = images.to(DEVICE, non_blocking=True)\\n                    idxs = idxs.to(DEVICE, non_blocking=True)\\n                    output = model(images)\\n\\n                    loss = criterion(output, idxs)\\n                    total_train_loss += loss.item()\"),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"loss.backward()\\n                    optimizer.step()\\n                    optimizer.zero_grad(set_to_none=True)\\n\\n            model.eval()\\n            with tqdm(valid_dataloader, unit='batch', leave=False) as pbar:\\n                pbar.set_description(f'testing')\\n                for images, idxs in pbar:\\n                    images = images.to(DEVICE, non_blocking=True)\\n                    idxs = idxs.to(DEVICE, non_blocking=True)\\n                    output = model(images)\"),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"loss = criterion(output, idxs)\\n                    total_test_loss += loss.item()\\n\\n            train_loss = total_train_loss / len(self.data_transformation_artifacts.train_transformed_object)\\n            valid_loss = total_test_loss / len(self.data_transformation_artifacts.valid_transformed_object)\\n            print(f'Train Loss: {train_loss:.4f} Test Loss: {valid_loss:.4f}')\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e\"),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='def initiate_model_trainer(self) -> ModelTrainerArtifacts:\\n        \"\"\"\\n        Method Name: initiate_model_trainer\\n        Description: This method initiate model trainer steps\\n\\n        Output: Return model trainer artifacts\\n        On Failur: Write an exception log and then raise an exception\\n        \"\"\"\\n        try:\\n            logging.info(\"Entered the initiate_model_trainer method of Model trainer class\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='train_dataset = load_object(self.data_transformation_artifacts.train_transformed_object)\\n            valid_dataset = load_object(self.data_transformation_artifacts.valid_transformed_object)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Loaded dataset from data transformation artifacts\")\\n            train_loader = DataLoader(train_dataset, shuffle=True, batch_size=self.batch_size, num_workers=self.num_workers)\\n            valid_loader = DataLoader(valid_dataset, shuffle=True, batch_size=self.batch_size, num_workers=self.num_workers)\\n            logging.info(\"Loaded train and valid data loader\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='model = models.resnet34(weights=\\'ResNet34_Weights.DEFAULT\\')\\n            logging.info(\"Loaded pretrained resnet34 model\")\\n\\n            model.fc = nn.Sequential(\\n                nn.Dropout(0.1),\\n                nn.Linear(model.fc.in_features, self.data_transformation_artifacts.classes)\\n            )\\n            logging.info(\"Updated the last layer of pretrained model\")\\n\\n            model = model.to(DEVICE)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='criterion = torch.nn.CrossEntropyLoss()\\n            logging.info(\"Cross entropy loss function is used.\")\\n\\n            optimizer = torch.optim.SGD(model.parameters(), lr=self.learning_rate, momentum=0.9)\\n            logging.info(\"SGD optimizer is used.\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Model Training Started\")\\n            for i in range(self.epochs):\\n                logging.info(f\"Model training at epoch: {i+1}\")\\n                print(f\"Epoch: {i+1}/{self.epochs}\")\\n                self.train(model, criterion, optimizer, train_loader, valid_loader)\\n            logging.info(\"Model Training Done!!!\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='os.makedirs(self.model_trainer_config.MODEL_TRAINER_ARTIFACTS_DIR, exist_ok=True)\\n            torch.save(model, self.model_trainer_config.TRAINED_MODEL_PATH)\\n            logging.info(f\"Saved trained model at {self.model_trainer_config.TRAINED_MODEL_PATH}\")\\n\\n            model_trainer_artifacts = ModelTrainerArtifacts(\\n                trained_model_path=self.model_trainer_config.TRAINED_MODEL_PATH)\\n            logging.info(f\"Model trainer artifacts: {model_trainer_artifacts}\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Exited the initiate_model_trainer method of Model trainer class\")\\n            return model_trainer_artifacts\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\configurations\\\\gcloud_syncer.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nfrom src.exception import CustomException'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\configurations\\\\gcloud_syncer.py', 'language': <Language.PYTHON: 'python'>}, page_content='class GCloudSync:\\n    def sync_file_from_gcloud(self, gcp_bucket_url, filename, destination):\\n        \"\"\"\\n        param gcp_bucket_url: GCP bucket url\\n        param filepath: filepath\\n        param destination: where to store\\n        \"\"\"\\n        try:\\n            command = f\"gsutil cp gs://{gcp_bucket_url}/{filename} {destination}/\"\\n            #command = f\"gcloud storage cp gs://{gcp_bucket_url}/{filename} {destination}/{filename}\"\\n            os.system(command)\\n        except Exception as e:'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\configurations\\\\gcloud_syncer.py', 'language': <Language.PYTHON: 'python'>}, page_content='raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\configurations\\\\gcloud_syncer.py', 'language': <Language.PYTHON: 'python'>}, page_content='def sync_file_to_gcloud(self, gcp_bucket_url, filepath):\\n        \"\"\"\\n        param gcp_bucket_url: GCP bucket url\\n        param filepath: filepath\\n        \"\"\"\\n        try:\\n            command = f\"gsutil cp {filepath} gs://{gcp_bucket_url}/\"\\n            #command = f\"gcloud storage cp {filepath}/{filename} gs://{gcp_bucket_url}/\"\\n            os.system(command)\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\constants\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport torch\\nfrom datetime import datetime\\n\\n#Common Constants\\n\\nCONFIG_PATH: str = os.path.join(os.getcwd(), \"config\", \"config.yaml\")\\nTIMESTAMP: str = datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\")\\nARTIFACTS_DIR = os.path.join(\"artifacts\", TIMESTAMP)\\nuse_cuda = torch.cuda.is_available()\\nDEVICE = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\\n\\n#FastAPI Constants\\nAPP_HOST = \"127.0.0.1\"\\nAPP_PORT =8080\\n\\n#Data Ingestion Constants\\nDATA_INGESTION_ARTIFACTS_DIR = \\'DataIngestionArtifacts\\''),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\constants\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='#Data Transformation Constants\\nDATA_TRANSFORMATION_ARTIFACTS_DIR = \\'DataTransformationArtifacts\\'\\nDATA_TRANSFORMATION_TRAIN_FILE_NAME = \"train_transformed.pkl\"\\nDATA_TRANSFORMATION_VALID_FILE_NAME = \"valid_transformed.pkl\"\\nDATA_TRANSFORMATION_TEST_FILE_NAME = \"test_transformed.pkl\"\\n\\n#Model Trainer Constants\\nMODEL_TRAINER_ARTIFACTS_DIR = \"ModelTrainerArtifacts\"\\nTRAINED_MODEL_PATH = \"model.pt\"'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\constants\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='#Model Evaluation Constants\\nMODEL_EVALUATION_ARTIFACTS_DIR = \"ModelEvaluationArtifacts\"\\nBEST_MODEL_DIR = \"best_model\"\\nMODEL_NAME = \"model.pt\"\\n\\n#Prediction Pipeline\\nLABEL_NAME = [\\'Forged\\', \\'Original\\']'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\artifacts_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='from dataclasses import dataclass\\n\\n#Data Ingestion Artifacts\\n@dataclass\\nclass DataIngestionArtifacts:\\n    dataset_path: str\\n\\n    def to_dict(self):\\n        return self.__dict__\\n\\n#Data Transformation Artifacts\\n@dataclass\\nclass DataTransformationArtifacts:\\n    train_transformed_object: str\\n    valid_transformed_object: str\\n    test_transformed_object: str\\n    classes: int\\n\\n    def to_dict(self):\\n        return self.__dict__\\n\\n#Model Trainer Artifacts\\n@dataclass'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\artifacts_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='class ModelTrainerArtifacts:\\n    trained_model_path: str\\n\\n    def to_dict(self):\\n        return self.__dict__\\n\\n#Model Evaluation Artifacts:\\n@dataclass\\nclass ModelEvaluationArtifacts:\\n    is_model_accepted: bool\\n\\n    def to_dict(self):\\n        return self.__dict__\\n\\n#Model Pusher Artifacts\\n@dataclass\\nclass ModelPusherArtifacts:\\n    bucket_name: str\\n\\n    def to_dict(self):\\n        return self.__dict__'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom src.constants import *\\nfrom dataclasses import dataclass\\nfrom src.utils.main_utils import read_yaml_file\\n\\n@dataclass'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='class DataIngestionConfig:\\n    def __init__(self):\\n        self.config = read_yaml_file(CONFIG_PATH)\\n        self.BUCKET_NAME: str = self.config[\\'data_ingestion_config\\'][\"bucket_name\"]\\n        self.ZIP_FILE_NAME: str = self.config[\\'data_ingestion_config\\'][\"zip_file_name\"]\\n        self.DATA_INGESTION_ARTIFACTS_DIR: str = os.path.join(os.getcwd(), ARTIFACTS_DIR, DATA_INGESTION_ARTIFACTS_DIR)\\n        self.ZIP_FILE_PATH: str = os.path.join(self.DATA_INGESTION_ARTIFACTS_DIR, self.ZIP_FILE_NAME)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='class DataTransformationConfig:\\n    def __init__(self):\\n        self.config = read_yaml_file(CONFIG_PATH)\\n        self.STD: list = self.config[\\'data_transformation_config\\'][\"std\"]\\n        self.MEAN: list = self.config[\\'data_transformation_config\\'][\"mean\"]\\n        self.IMG_SIZE: int = self.config[\\'data_transformation_config\\'][\"img_size\"]\\n        self.DEGREE_N: int = self.config[\\'data_transformation_config\\'][\"degree_n\"]'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='self.DEGREE_P: int = self.config[\\'data_transformation_config\\'][\"degree_p\"]\\n        self.TRAIN_RATIO: float = self.config[\\'data_transformation_config\\'][\"train_ratio\"]\\n        self.VALID_RATIO: float = self.config[\\'data_transformation_config\\'][\"valid_ratio\"]\\n        self.DATA_TRANSFORMATION_ARTIFACTS_DIR: str = os.path.join(os.getcwd(), ARTIFACTS_DIR, DATA_TRANSFORMATION_ARTIFACTS_DIR)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='self.TRAIN_TRANSFORM_OBJECT_FILE_PATH: str = os.path.join(self.DATA_TRANSFORMATION_ARTIFACTS_DIR, DATA_TRANSFORMATION_TRAIN_FILE_NAME)\\n        self.VALID_TRANSFORM_OBJECT_FILE_PATH: str = os.path.join(self.DATA_TRANSFORMATION_ARTIFACTS_DIR, DATA_TRANSFORMATION_VALID_FILE_NAME)\\n        self.TEST_TRANSFORM_OBJECT_FILE_PATH: str = os.path.join(self.DATA_TRANSFORMATION_ARTIFACTS_DIR, DATA_TRANSFORMATION_TEST_FILE_NAME)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='@dataclass'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"class ModelTrainerConfig:\\n    def __init__(self):\\n        self.config = read_yaml_file(CONFIG_PATH)\\n        self.LR: float = self.config['model_trainer_config']['lr']\\n        self.EPOCHS: int = self.config['model_trainer_config']['epochs']\\n        self.NUM_WORKERS: int = self.config['model_trainer_config']['num_workers']\\n        self.BATCH_SIZE: int = self.config['model_trainer_config']['batch_size']\"),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='self.MODEL_TRAINER_ARTIFACTS_DIR: str = os.path.join(os.getcwd(), ARTIFACTS_DIR, MODEL_TRAINER_ARTIFACTS_DIR)\\n        self.TRAINED_MODEL_PATH: str = os.path.join(self.MODEL_TRAINER_ARTIFACTS_DIR, TRAINED_MODEL_PATH)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='@dataclass'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='class ModelEvaluationConfig:\\n    def __init__(self):\\n        self.config = read_yaml_file(CONFIG_PATH)\\n        self.MODEL_NAME: str = MODEL_NAME\\n        self.BUCKET_NAME: str = self.config[\\'model_evaluation_config\\'][\"bucket_name\"]\\n        self.BATCH_SIZE: int = self.config[\\'model_evaluation_config\\'][\"batch_size\"]\\n        self.NUM_WORKERS: int = self.config[\\'model_evaluation_config\\'][\"num_workers\"]'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='self.MODEL_EVALUATION_ARTIFACTS_DIR: str = os.path.join(os.getcwd(), ARTIFACTS_DIR, MODEL_EVALUATION_ARTIFACTS_DIR)\\n        self.BEST_MODEL_DIR: str = os.path.join(self.MODEL_EVALUATION_ARTIFACTS_DIR, BEST_MODEL_DIR)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='@dataclass'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\entity\\\\config_entity.py', 'language': <Language.PYTHON: 'python'>}, page_content='class ModelPusherConfig:\\n    def __init__(self):\\n        self.config = read_yaml_file(CONFIG_PATH)\\n        self.MODEL_NAME: str = MODEL_NAME\\n        self.BUCKET_NAME: str = self.config[\\'model_pusher_config\\'][\"bucket_name\"]'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\exception\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='import sys\\n\\n\\ndef error_message_detail(error, error_details: sys):\\n    _, _, exc_tb = error_details.exc_info()\\n    file_name = exc_tb.tb_frame.f_code.co_filename\\n    error_message = \"Error occurred python script name [{0}] line number [{1}] error message[{2}]\".format(\\n        file_name, exc_tb.tb_lineno, str(error)\\n    )\\n\\n    return error_message'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\exception\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='class CustomException(Exception):\\n    def __init__(self, error_message, error_detail):\\n        super().__init__(error_message)\\n        self.error_message = error_message_detail(\\n            error_message, error_details=error_detail\\n        )\\n\\n    def __str__(self):\\n        return self.error_message'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\logger\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='import logging\\nimport os\\nfrom datetime import datetime\\n\\nLOG_FILE_NAME = f\"{datetime.now().strftime(\\'%m_%d_%Y_%H_%M_%S\\')}.log\"\\n\\nos.makedirs(os.path.join(os.getcwd(), \"logs\"), exist_ok=True)\\n\\nlogs_dir_path = os.path.join(os.getcwd(), \"logs\")\\n\\nLOG_FILE_PATH = os.path.join(logs_dir_path, LOG_FILE_NAME)\\n\\nlogging.basicConfig(\\n    filename=LOG_FILE_PATH,\\n    format=\"[ %(asctime)s ] %(name)s - %(levelname)s - %(message)s\",\\n    level=logging.INFO,\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport torch\\nfrom PIL import Image\\nfrom src.constants import *\\nfrom src.logger import logging\\nfrom torchvision import transforms\\nfrom src.exception import CustomException\\nfrom src.utils.main_utils import read_yaml_file\\nfrom src.configurations.gcloud_syncer import GCloudSync'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"class PredictionPipeline:\\n\\n    def __init__(self):\\n        self.gcloud = GCloudSync()\\n        self.config = read_yaml_file(CONFIG_PATH)\\n        self.img_size = self.config['data_transformation_config']['img_size']\"),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='def image_loader(self, image_bytes):\\n        \"\"\"\\n        Method Name: Image loader\\n        Description: This method load byte image and save it to local\\n        Output: Returns path of the saved image        :\\n        \"\"\"\\n        logging.info(\"Entered the image_loader method of PredictionPipeline class\")\\n        try:\\n            logging.info(\"load byte image and save it to local\")\\n            input_image = self.config[\\'prediction_pipeline_config\\'][\\'input_image\\']'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='with open(input_image, \\'wb\\') as image:\\n                image.write(image_bytes)\\n                image.close()\\n            path = os.path.join(os.getcwd(), input_image)\\n            image = Image.open(path)\\n            logging.info(f\"Returns the saved image: {image}\")\\n            logging.info(\"Exited the image_loader method of PredictionPipeline class\")\\n            return image\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='def get_model_from_gcloud(self) -> str:\\n        \"\"\"\\n        Method Name: Get_model_from_gcloud\\n        Description: This method fetched the best model from the gcloud\\n        Output: Return best model path\\n        \"\"\"\\n        logging.info(\"Entered the get_model_from_gcloud method of PredictionPipeline class\")\\n        try:\\n            logging.info(\"Loading the best model from gcloud bucket\")\\n            os.makedirs(\"artifacts/PredictModel\", exist_ok=True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='predict_model_path = os.path.join(os.getcwd(), \"artifacts\", \"PredictModel\")\\n            self.gcloud.sync_file_from_gcloud(self.config[\\'prediction_pipeline_config\\'][\"bucket_name\"],\\n                                              self.config[\\'prediction_pipeline_config\\'][\"model_name\"],\\n                                              predict_model_path)\\n            best_model_path = os.path.join(predict_model_path, self.config[\\'prediction_pipeline_config\\'][\"model_name\"])'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Exited the get_model_from_gcloud method of PredictionPipeline class\")\\n            return best_model_path'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='def prediction(self, best_model_path: str, image) -> float:\\n        \"\"\"\\n        Method Name: Prediction\\n        Description: This method takes the best model path and image\\n        Output: Return the image in base64\\n        \"\"\"\\n        logging.info(\"Entered the prediction method of PredictionPipeline class\")\\n        try:\\n            logging.info(\"Loading best model\")\\n            model = torch.load(best_model_path, map_location=DEVICE)\\n            model.eval()'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Load the image and preprocess it\")\\n            preprocess = transforms.Compose([\\n                transforms.Resize(size=(self.img_size, self.img_size)),\\n                transforms.Grayscale(3),\\n                transforms.ToTensor()\\n            ])\\n            image = preprocess(image)\\n            image = image[:3]\\n\\n            logging.info(\"Convert image to a PyTorch tensor and sent it to the device\")\\n            image = image.unsqueeze(0).to(DEVICE)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Make the prediction\")\\n            with torch.no_grad():\\n                logits = model(image)\\n                probs = torch.softmax(logits, dim=1)\\n                pred_label = torch.argmax(probs, dim=1)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(f\\'Predicted label: {pred_label.item()}\\')\\n            logging.info(\"Map the predicted label to the corresponding class name\")\\n            predicted_class_name = LABEL_NAME[pred_label.item()]\\n            logging.info(f\\'Predicted class name: {predicted_class_name}\\')\\n            logging.info(\"Exited the prediction method of PredictionPipeline class\")\\n            return predicted_class_name\\n\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='def run_pipeline(self, data):\\n        logging.info(\"Entered the run_pipeline method of PredictionPipeline class\")\\n        try:\\n            image = self.image_loader(data)\\n            best_model_path: str = self.get_model_from_gcloud()\\n            detected_image = self.prediction(best_model_path, image)\\n            logging.info(\"Exited the run_pipeline method of PredictionPipeline class\")\\n            return detected_image'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nfrom src.logger import logging\\nfrom src.exception import CustomException\\nfrom src.components.data_ingestion import DataIngestion\\nfrom src.components.data_transformation import DataTransformation\\nfrom src.components.model_trainer import ModelTrainer\\nfrom src.components.model_evaluation import ModelEvaluation\\nfrom src.components.model_pusher import ModelPusher'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.entity.config_entity import DataIngestionConfig, DataTransformationConfig, ModelTrainerConfig, ModelEvaluationConfig, ModelPusherConfig\\nfrom src.entity.artifacts_entity import DataIngestionArtifacts, DataTransformationArtifacts, ModelTrainerArtifacts, ModelEvaluationArtifacts, ModelPusherArtifacts'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='class TrainingPipeline:\\n    def __init__(self):\\n        self.data_ingestion_config = DataIngestionConfig()\\n        self.data_transformation_config = DataTransformationConfig()\\n        self.model_trainer_config = ModelTrainerConfig()\\n        self.model_evaluation_config = ModelEvaluationConfig()\\n        self.model_pusher_config = ModelPusherConfig()'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='def start_data_ingestion(self) -> DataIngestionArtifacts:\\n        logging.info(\"Entered the start_data_ingestion method of TrainingPipeline class\")\\n        try:\\n            logging.info(\"Getting the dataset from GCloud Storage Bucket\")\\n            data_ingestion = DataIngestion(\\n                data_ingestion_config=self.data_ingestion_config\\n            )\\n            data_ingestion_artifacts = data_ingestion.initiate_data_ingestion()'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Got the dataset from GCloud Storage\")\\n            logging.info(\"Exited the start_data_ingestion method of TrainingPipeline class\")\\n            return data_ingestion_artifacts\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='def start_data_transformation(self, data_ingestion_artifacts: DataIngestionArtifacts) -> DataTransformationArtifacts:\\n        logging.info(\"Entered the start_data_transformation method of TrainingPipeline class\")\\n        try:\\n            data_transformation = DataTransformation(\\n                data_ingestion_artifacts=data_ingestion_artifacts,\\n                data_transformation_config=self.data_transformation_config,\\n            )'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content=')\\n            data_transformation_artifacts = (data_transformation.initiate_data_transformation())\\n            logging.info(\"Exited the start_data_transformation method of TrainingPipeline class\")\\n            return data_transformation_artifacts\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='def start_model_trainer(self, data_transformation_artifacts: DataTransformationArtifacts) -> ModelTrainerArtifacts:\\n        logging.info(\"Entered the start_model_trainer method of TrainingPipeline class\")\\n        try:\\n            model_trainer = ModelTrainer(\\n                data_transformation_artifacts=data_transformation_artifacts,\\n                model_trainer_config = self.model_trainer_config,\\n            )'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content=')\\n            model_trainer_artifacts = (model_trainer.initiate_model_trainer())\\n            logging.info(\"Exited the start_model_trainer method of TrainingPipeline class\")\\n            return model_trainer_artifacts\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='def start_model_evaluation(self, model_trainer_artifacts: ModelTrainerArtifacts,\\n                               data_transformation_artifacts: DataTransformationArtifacts) -> ModelEvaluationArtifacts:\\n        logging.info(\"Entered the start_model_evaluation method of TrainingPipeline class\")\\n        try:\\n            model_evaluation = ModelEvaluation(\\n                model_evaluation_config=self.model_evaluation_config,'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='data_transformation_artifacts=data_transformation_artifacts,\\n                model_trainer_artifacts=model_trainer_artifacts\\n            )\\n            model_evaluation_artifacts = model_evaluation.initiate_model_evaluation()\\n            logging.info(\"Exited the start_model_evaluation method of TrainingPipeline class\")\\n            return model_evaluation_artifacts'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='def start_model_pusher(self, model_trainer_artifacts: ModelTrainerArtifacts) -> ModelPusherArtifacts:\\n        logging.info(\"Entered the start_model_pusher method of TrainingPipeline class\")\\n        try:\\n            model_pusher = ModelPusher(model_pusher_config=self.model_pusher_config, model_trainer_artifacts=model_trainer_artifacts)\\n            model_pusher_artifacts = model_pusher.initiate_model_pusher()\\n            logging.info(\"Initiated the model pusher\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='logging.info(\"Exited the start_model_pusher method of TrainingPipeline class\")\\n            return model_pusher_artifacts\\n        except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='def run_pipeline(self) -> None:\\n        logging.info(\"Entered the run_pipeline method of TrainingPipeline class\")\\n        try:\\n            data_ingestion_artifacts = self.start_data_ingestion()\\n            data_transformation_artifacts = self.start_data_transformation(data_ingestion_artifacts=data_ingestion_artifacts)\\n            model_trainer_artifacts = self.start_model_trainer(data_transformation_artifacts=data_transformation_artifacts)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='model_evaluation_artifacts =self.start_model_evaluation(model_trainer_artifacts=model_trainer_artifacts, data_transformation_artifacts=data_transformation_artifacts)\\n            if not model_evaluation_artifacts.is_model_accepted:\\n                raise Exception(\"Trained model is not better than the best model\")\\n            model_pusher_artifacts = self.start_model_pusher(model_trainer_artifacts=model_trainer_artifacts)'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\pipeline\\\\training.py', 'language': <Language.PYTHON: 'python'>}, page_content='except Exception as e:\\n            raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\utils\\\\main_utils.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport dill\\nimport yaml\\nimport base64\\nfrom src.logger import logging\\nfrom src.exception import CustomException'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\utils\\\\main_utils.py', 'language': <Language.PYTHON: 'python'>}, page_content='def save_object(file_path: str, obj: object) -> None:\\n    logging.info(\"Entered the save object method of utils\")\\n    try:\\n        os.makedirs(os.path.dirname(file_path),exist_ok=True)\\n        with open(file_path, \"wb\") as file_obj:\\n            dill.dump(obj, file_obj)\\n        logging.info(\"Exited the save_object method of utils\")\\n    except Exception as e:\\n        raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\utils\\\\main_utils.py', 'language': <Language.PYTHON: 'python'>}, page_content='def load_object(file_path: str) -> object:\\n    logging.info(\"Entered the load_object method of utils\")\\n    try:\\n        with open(file_path, \"rb\") as file_obj:\\n            obj = dill.load(file_obj)\\n        logging.info(\"Exited the load_object method of utils\")\\n        return obj\\n    except Exception as e:\\n        raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\utils\\\\main_utils.py', 'language': <Language.PYTHON: 'python'>}, page_content='def image_to_base64(image):\\n    try:\\n        logging.info(\"Entered the image_to_base64 method of utils\")\\n        with open(image, \"rb\") as img_file:\\n            my_string = base64.b64encode(img_file.read())\\n        logging.info(\"Exited the image_to_base64 method of utils\")\\n        return my_string\\n    except Exception as e:\\n        raise CustomException(e, sys) from e'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\utils\\\\main_utils.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"def read_yaml_file(file_path: str) -> dict:\\n    try:\\n        with open(file_path, 'rb') as yaml_file:\\n            return yaml.safe_load(yaml_file)\\n    except Exception as e:\\n        raise CustomException(e, sys) from e\")]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "GROQ_API_KEY = os.environ[\"GROQ_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lokesh\\AppData\\Local\\Temp\\ipykernel_14956\\3987696487.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})\n",
      "c:\\Users\\Lokesh\\anaconda3\\envs\\sourcecode\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings_model, persist_directory='.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lokesh\\AppData\\Local\\Temp\\ipykernel_14956\\3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key= \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is evaluate function?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `evaluate` function is to evaluate a model using a given loss function and data loader. It takes a model, a criterion (loss function), and a test data loader as input, and likely calculates the loss of the model on the test data.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is get_data_from_gcloud function?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the `get_data_from_gcloud` function is to fetch data from a GCloud Storage Bucket. It returns data ingestion artifacts.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sourcecode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
